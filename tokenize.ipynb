{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcb8a80-6125-47d9-9e03-e614ea8e17fc",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65846a34-2ffd-4c07-aff3-c42df0fbf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir=\"/scratch/cse/phd/anz198717/XC\"\n",
    "corpus_dataset=\"AmazonTitles-1.3M-dummy\"\n",
    "txt_model=\"sentencebert\"\n",
    "corpus_dset=f\"{work_dir}/Corpus/{corpus_dataset}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0cbb83-d18a-470d-8136-a16c9440e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import site\n",
    "import sys\n",
    "site.addsitedir(f\"{work_dir}/programs/ExtremeMethods\")\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import xc.tools.build_from_msr as msr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cca8fc-b8d0-48ac-af6d-c35b155a72c1",
   "metadata": {},
   "source": [
    "## Only for internal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1e2a97-7ef3-425c-9bac-424bc430f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=f\"--in_dir {corpus_dset} --ot_dir {corpus_dset} \\\n",
    "#     --docs_input corpus_data.txt --lbls_input corpus_x_y.txt\"\n",
    "# sys.argv = f\"TOKEN {args}\".split()\n",
    "# print(args)\n",
    "# args = msr.setup()\n",
    "# lines = msr.build_docs(args)\n",
    "# msr.build_lbls(args, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4619b8f7-c45d-4c4c-912c-93710345acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=f\"{corpus_dset}/temp\"\n",
    "img_path=f\"{corpus_dset}/img.bin\"\n",
    "tst_map=f\"{corpus_dset}/test_map.txt\"\n",
    "lbl_map=f\"{corpus_dset}/label_map.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1edb1-3154-40a2-9faa-22149abe08e7",
   "metadata": {},
   "source": [
    "## Tokenize and Build data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3092198e-94f5-4ed8-b6bb-021e812fc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xc.tools.tokenize_text as token\n",
    "from tqdm.notebook import tqdm\n",
    "from xc.libs.utils import pbar\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "sys.argv = f\"XCSAGE\".split()\n",
    "\n",
    "from xc.libs.data_img import IMGBINDataset, read_raw_img_bin\n",
    "from xc.libs.custom_dtypes import FeaturesAccumulator\n",
    "from xc.libs.dataparallel import DataParallel\n",
    "from xc.models.models_img import Model\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Pretrained models')\n",
    "args = parser.parse_args()\n",
    "args.project_dim = -1\n",
    "args.corpus_dir = corpus_dset\n",
    "args.batch_size = 256\n",
    "\n",
    "\n",
    "def tokens(text_map, _tokenizer, max_len=32):\n",
    "    text = list(map(lambda x: x.strip().split(\"->\", 1)[1],\n",
    "                    pbar(open(text_map, \"r\", encoding=\"latin1\"), desc=\"docs\")))\n",
    "    text = _tokenizer(text, truncation=True, padding=True,\n",
    "                      max_length=max_len, add_special_tokens=True,)\n",
    "    input_idx = np.asarray(text.input_ids, dtype=np.int32)\n",
    "    attention = np.asarray(text.attention_mask, dtype=np.int32)\n",
    "    max_vocab = _tokenizer.vocab_size\n",
    "    _tokens = np.stack([input_idx, attention], axis=1)\n",
    "    return _tokens, max_vocab\n",
    "\n",
    "\n",
    "def read_ptrs(file):\n",
    "    ptrs = {}\n",
    "    curr = 0\n",
    "    if os.path.exists(file):\n",
    "        with open(file, \"rb\") as f:\n",
    "            for line in pbar(f):\n",
    "                uid, _ = line.split(b\"\\t\", 1)\n",
    "                uid = uid.decode('utf-8')\n",
    "                _ptrs = ptrs.get(uid, [])\n",
    "                _ptrs.append(curr)\n",
    "                ptrs[uid] = _ptrs\n",
    "                curr = f.tell()\n",
    "    return ptrs\n",
    "\n",
    "def build_sparse_mat(doc_map, dict_ptrs):\n",
    "    uids = list(map(lambda x: x.split(\"->\", 1)[0], pbar(open(doc_map,\"r\", encoding=\"latin1\"))))\n",
    "    ptrs, cols, rows, num_cols, num_rows = [], [], [], 0, 0\n",
    "    for row, uid in pbar(enumerate(uids), desc=\"buildling\"):\n",
    "        uid = uid.split(\",\")\n",
    "        # NOTE offesting it with 1 for sparse matrix\n",
    "        sub_ptrs = np.concatenate(\n",
    "            list(map(lambda x: dict_ptrs.get(x, [-1]), uid))) + 1\n",
    "        ptrs.append(sub_ptrs)\n",
    "        cols.append(np.arange(sub_ptrs.size) + num_cols)\n",
    "        rows.append(np.ones(sub_ptrs.size)*row)\n",
    "        num_cols += sub_ptrs.size\n",
    "    image_mat = sp.lil_matrix((len(uids), num_cols))\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "    ptrs = np.concatenate(ptrs)\n",
    "    image_mat[rows, cols] = ptrs\n",
    "    image_mat = image_mat.tocsr()\n",
    "    return image_mat\n",
    "\n",
    "def save(data_path, file_name, img, txt):\n",
    "    suffix=os.path.join(data_path, file_name)\n",
    "    if img.nnz >0:\n",
    "        sp.save_npz(f\"{suffix}.img.bin.npz\", img)\n",
    "    txt, max_vocab = txt\n",
    "    data = np.memmap(f\"{suffix}.txt.seq.memmap.dat\", dtype=np.int32, mode=\"w+\", shape=txt.shape)\n",
    "    data[:] = txt\n",
    "    data.flush()\n",
    "    inst, channel, length = txt.shape\n",
    "    with open(f\"{suffix}.txt.seq.memmap.meta\", \"w\") as f:\n",
    "        f.write(f\"{inst},{channel},{length},{max_vocab}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405abd0e-2ba4-4677-ba92-20bba291a956",
   "metadata": {},
   "source": [
    "## Fetching raw vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961b0499-94b3-4e3b-bf86-d950d28c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = torch.cat(list(map(lambda x: x.get_raw_vect(), batch)), dim=0)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def get_pre_trained(model, img_file, params):\n",
    "    pre_trained = Model(model, params)\n",
    "    pre_trained = DataParallel(pre_trained)\n",
    "    dataset = IMGBINDataset(params.corpus_dir, img_file, \"img.bin\", random_k=-1)\n",
    "    # dataset.read_func = read_raw_img_bin\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=params.batch_size, collate_fn=collate_fn,\n",
    "        shuffle=False, num_workers=6, prefetch_factor=2)\n",
    "    features = FeaturesAccumulator(\"Image features\", \"memmap\", \".img.vect\")\n",
    "    mask = None\n",
    "    with torch.no_grad():\n",
    "        pre_trained = pre_trained.cuda()\n",
    "        pre_trained = pre_trained.eval()\n",
    "        start = 0\n",
    "        with torch.cuda.amp.autocast():\n",
    "            for idx, data in enumerate(tqdm(dl)):\n",
    "                embs, mask = pre_trained(data)\n",
    "                features.transform(embs, mask)\n",
    "    features.compile()\n",
    "    features.remap(dataset.data)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce44e770-fb44-46f3-b170-1bf65d60b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docs: 970237it [00:00, 1087165.72it/s]\n",
      "docs: 1305265it [00:01, 1074220.80it/s]\n",
      "970237it [00:00, 1296265.58it/s]\n",
      "buildling: 970237it [00:15, 64480.39it/s]\n",
      "1305265it [00:00, 1336204.46it/s]\n",
      "buildling: 1305265it [00:20, 63698.07it/s]\n"
     ]
    }
   ],
   "source": [
    "_tokenizer = token.AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v4\", do_lower_case=True)\n",
    "dict_ptrs = read_ptrs(img_path)\n",
    "\n",
    "tst_txt = tokens(tst_map, _tokenizer)\n",
    "lbl_txt = tokens(lbl_map, _tokenizer)\n",
    "\n",
    "tst_img = build_sparse_mat(tst_map, dict_ptrs)\n",
    "lbl_img = build_sparse_mat(lbl_map, dict_ptrs)\n",
    "\n",
    "\n",
    "os.makedirs(f\"{data_dir}\", exist_ok=True)\n",
    "save(f\"{data_dir}\", \"test\", tst_img, tst_txt)\n",
    "save(f\"{data_dir}\", \"label\", lbl_img, lbl_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc5a8b7-cd9d-40ea-bdb4-b7b1e6867978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docs: 2248619it [00:02, 1096099.53it/s]\n",
      "2248619it [00:01, 1351008.50it/s]\n",
      "buildling: 2248619it [00:34, 64649.83it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_txt = tokens(f\"{corpus_dset}/train_map.txt\", _tokenizer)\n",
    "trn_img = build_sparse_mat(f\"{corpus_dset}/train_map.txt\", dict_ptrs)\n",
    "save(f\"{data_dir}\", \"train\", trn_img, trn_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b52074-9da5-4e5c-b198-fa89e08c6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_vect = get_pre_trained(\"ViT\", f\"{data_dir}/test.img.bin.npz\", args)\n",
    "tst_vect.save(os.path.join(data_dir, \"ViT/test\"))\n",
    "\n",
    "lbl_vect = get_pre_trained(\"ViT\", f\"{data_dir}/label.img.bin.npz\", args)\n",
    "lbl_vect.save(os.path.join(data_dir, \"ViT/label\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
